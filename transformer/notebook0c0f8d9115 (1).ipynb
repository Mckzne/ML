{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-31T15:55:51.224493Z",
     "iopub.status.busy": "2025-03-31T15:55:51.224197Z",
     "iopub.status.idle": "2025-03-31T15:55:51.531827Z",
     "shell.execute_reply": "2025-03-31T15:55:51.531104Z",
     "shell.execute_reply.started": "2025-03-31T15:55:51.224469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pretrained/distilbert_step_15000.pt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:55:54.996048Z",
     "iopub.status.busy": "2025-03-31T15:55:54.995560Z",
     "iopub.status.idle": "2025-03-31T15:55:58.261366Z",
     "shell.execute_reply": "2025-03-31T15:55:58.259904Z",
     "shell.execute_reply.started": "2025-03-31T15:55:54.996018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:55:58.668499Z",
     "iopub.status.busy": "2025-03-31T15:55:58.668184Z",
     "iopub.status.idle": "2025-03-31T15:56:03.341790Z",
     "shell.execute_reply": "2025-03-31T15:56:03.341123Z",
     "shell.execute_reply.started": "2025-03-31T15:55:58.668470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:03.343370Z",
     "iopub.status.busy": "2025-03-31T15:56:03.342863Z",
     "iopub.status.idle": "2025-03-31T15:56:10.399751Z",
     "shell.execute_reply": "2025-03-31T15:56:10.399054Z",
     "shell.execute_reply.started": "2025-03-31T15:56:03.343338Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmihika-usgaonker\u001b[0m (\u001b[33mmihika-usgaonker-none\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "wan = user_secrets.get_secret(\"wandb_api\")\n",
    "wandb.login(key=wan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load TinyStories dataset\n",
    "#dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Print dataset structure\n",
    "#print(dataset)\n",
    "#print(dataset[\"train\"][0])  # Print the first sample\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_CHECKPOINT = \"distilbert-base-uncased\"  # Using DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Print a sample\n",
    "print(tokenized_datasets[\"train\"][0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "def mask_tokens(inputs, mlm_probability=0.15):\n",
    "    # Only print for the first batch processed\n",
    "    if not hasattr(mask_tokens, \"printed_first_batch\"):\n",
    "        # Print original input before masking (numerical form)\n",
    "        print(\"Original Input IDs (first sequence):\")\n",
    "        print(inputs[0].tolist())\n",
    "        \n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for masked-LM training (with probability `mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(0, tokenizer.vocab_size, labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the original token\n",
    "        \n",
    "        # Print masked input after masking (numerical form)\n",
    "        print(\"Masked Input IDs (first sequence):\")\n",
    "        print(inputs[0].tolist())\n",
    "        \n",
    "        # Print which tokens were masked (showing only the first sequence)\n",
    "        print(\"Masked positions (first sequence):\")\n",
    "        first_seq_masked = masked_indices[0].tolist()\n",
    "        masked_positions = [i for i, mask in enumerate(first_seq_masked) if mask]\n",
    "        print(f\"Positions: {masked_positions}\")\n",
    "        \n",
    "        # Print labels for the first sequence\n",
    "        print(\"Labels for first sequence (original token IDs at masked positions, -100 elsewhere):\")\n",
    "        print(labels[0].tolist())\n",
    "        \n",
    "        # Set the flag to avoid printing for subsequent batches\n",
    "        mask_tokens.printed_first_batch = True\n",
    "        \n",
    "        return inputs, labels\n",
    "    else:\n",
    "        # For all other batches, just do the masking without printing\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(0, tokenizer.vocab_size, labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "# Apply masking to the dataset\n",
    "def mask_dataset(examples):\n",
    "    inputs, labels = mask_tokens(examples[\"input_ids\"])\n",
    "    return {\"input_ids\": inputs, \"labels\": labels, \"attention_mask\": examples[\"attention_mask\"]}\n",
    "\n",
    "masked_datasets = tokenized_datasets.map(mask_dataset, batched=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#wandb.init(project=\"not-distilbert-pretraining\", name=\"not-distilbert-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:18.069857Z",
     "iopub.status.busy": "2025-03-31T15:56:18.069214Z",
     "iopub.status.idle": "2025-03-31T15:56:18.093919Z",
     "shell.execute_reply": "2025-03-31T15:56:18.093035Z",
     "shell.execute_reply.started": "2025-03-31T15:56:18.069810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:21.361763Z",
     "iopub.status.busy": "2025-03-31T15:56:21.361406Z",
     "iopub.status.idle": "2025-03-31T15:56:21.374946Z",
     "shell.execute_reply": "2025-03-31T15:56:21.374019Z",
     "shell.execute_reply.started": "2025-03-31T15:56:21.361736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads  # Critical bug fix\n",
    "        self.attention_head_size = hidden_size // num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        batch_size, seq_length = x.size(0), x.size(1)\n",
    "        x = x.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_length = hidden_states.size(0), hidden_states.size(1)\n",
    "        \n",
    "        query = self.query(hidden_states)\n",
    "        key = self.key(hidden_states)\n",
    "        value = self.value(hidden_states)\n",
    "        \n",
    "        query = self.transpose_for_scores(query)\n",
    "        key = self.transpose_for_scores(key)\n",
    "        value = self.transpose_for_scores(value)\n",
    "        \n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "        \n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        context = torch.matmul(attention_probs, value)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        context = context.view(batch_size, seq_length, self.all_head_size)\n",
    "        \n",
    "        output = self.output(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.dense2 = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense1(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        hidden_states = self.dense2(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, intermediate_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(hidden_size, num_attention_heads, dropout_rate)\n",
    "        self.feed_forward = FeedForward(hidden_size, intermediate_size, dropout_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        hidden_states = self.layer_norm1(hidden_states + attention_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(hidden_states)\n",
    "        hidden_states = self.layer_norm2(hidden_states + ff_output)\n",
    "        \n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:25.425114Z",
     "iopub.status.busy": "2025-03-31T15:56:25.424805Z",
     "iopub.status.idle": "2025-03-31T15:56:25.433398Z",
     "shell.execute_reply": "2025-03-31T15:56:25.432487Z",
     "shell.execute_reply.started": "2025-03-31T15:56:25.425090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DistilBertModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_attention_heads, \n",
    "                 intermediate_size, max_position_embeddings, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_size, num_attention_heads, intermediate_size, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.mlm_head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        \n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        inputs_embeds = self.embeddings(input_ids)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        \n",
    "        embeddings = inputs_embeds + position_embeds\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        hidden_states = embeddings\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        \n",
    "        prediction_scores = self.mlm_head(hidden_states)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.mlm_head.out_features), \n",
    "                                      labels.view(-1))\n",
    "            loss = masked_lm_loss\n",
    "            \n",
    "        return {\"loss\": loss, \"logits\": prediction_scores} if loss is not None else prediction_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create model with configuration similar to distilbert-base-uncased\n",
    "model = DistilBERT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=768,\n",
    "    num_layers=6,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=512,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Adjust batch size based on your GPU memory\n",
    "batch_size = 8  # Start with a smaller batch size for Kaggle\n",
    "train_dataloader = DataLoader(\n",
    "    train_subset if 'train_subset' in locals() else masked_datasets[\"train\"], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Set up learning rate scheduler with warmup\n",
    "num_epochs = 1  # Adjust based on your time constraints\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * total_steps), \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Log model architecture to wandb\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Validation dataloader\n",
    "validation_dataloader = DataLoader(\n",
    "    masked_datasets[\"validation\"], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Set up training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "log_interval = 50  # Log more frequently on Kaggle\n",
    "accumulation_steps = 4  # Accumulate gradients over multiple batches\n",
    "save_interval = 5000  # Save model every 5000 steps\n",
    "\n",
    "# Create directory for saving model checkpoints\n",
    "os.makedirs(\"/kaggle/working/model_checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Get batch and move to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights every accumulation_steps batches\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update metrics\n",
    "            global_step += 1\n",
    "\n",
    "            # Log to wandb\n",
    "            if global_step % log_interval == 0:\n",
    "                wandb.log({\n",
    "                    \"train/loss\": loss.item() * accumulation_steps,  # Rescale loss\n",
    "                    \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                    \"train/global_step\": global_step\n",
    "                })\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                validation_loss = evaluate(model, validation_dataloader)\n",
    "                wandb.log({\n",
    "                    \"val/loss\": validation_loss\n",
    "                })\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if global_step % save_interval == 0:\n",
    "                checkpoint_path = f\"/kaggle/working/model_checkpoints/distilbert_step_{global_step}.pt\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'global_step': global_step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': loss.item() * accumulation_steps,\n",
    "                }, checkpoint_path)\n",
    "                wandb.save(checkpoint_path)\n",
    "                print(f\"Checkpoint saved at step {global_step}\")\n",
    "\n",
    "        # Update progress bar\n",
    "        epoch_loss += loss.item() * accumulation_steps\n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": loss.item() * accumulation_steps,\n",
    "            \"lr\": scheduler.get_last_lr()[0]\n",
    "        })\n",
    "\n",
    "    # Log epoch metrics\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    wandb.log({\n",
    "        \"train/epoch\": epoch + 1,\n",
    "        \"train/epoch_loss\": avg_epoch_loss\n",
    "    })\n",
    "\n",
    "    # Save model after each epoch\n",
    "    checkpoint_path = f\"/kaggle/working/model_checkpoints/distilbert_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': avg_epoch_loss,\n",
    "    }, checkpoint_path)\n",
    "    wandb.save(checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "# Save final model with configuration\n",
    "final_model_path = \"/kaggle/working/distilbert_pretrained_final.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'hidden_size': 768,\n",
    "    'num_layers': 6,\n",
    "    'num_attention_heads': 12,\n",
    "    'intermediate_size': 3072,\n",
    "    'max_position_embeddings': 512,\n",
    "    'config': {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'hidden_size': 768,\n",
    "        'num_layers': 6,\n",
    "        'num_attention_heads': 12,\n",
    "        'intermediate_size': 3072,\n",
    "        'max_position_embeddings': 512,\n",
    "        'dropout_rate': 0.1\n",
    "    }\n",
    "}, final_model_path)\n",
    "wandb.save(final_model_path)\n",
    "\n",
    "print(f\"Pretraining completed! Model saved to {final_model_path}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:34.041029Z",
     "iopub.status.busy": "2025-03-31T15:56:34.040655Z",
     "iopub.status.idle": "2025-03-31T15:56:37.139416Z",
     "shell.execute_reply": "2025-03-31T15:56:37.138671Z",
     "shell.execute_reply.started": "2025-03-31T15:56:34.040999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import DistilBertConfig, DistilBertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:51.487298Z",
     "iopub.status.busy": "2025-03-31T15:56:51.486562Z",
     "iopub.status.idle": "2025-03-31T15:56:51.775687Z",
     "shell.execute_reply": "2025-03-31T15:56:51.774981Z",
     "shell.execute_reply.started": "2025-03-31T15:56:51.487261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"/kaggle/working/fine_tuned_distilbert\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:56.333480Z",
     "iopub.status.busy": "2025-03-31T15:56:56.333164Z",
     "iopub.status.idle": "2025-03-31T15:56:56.549801Z",
     "shell.execute_reply": "2025-03-31T15:56:56.548868Z",
     "shell.execute_reply.started": "2025-03-31T15:56:56.333457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = DistilBertConfig.from_pretrained(MODEL_CHECKPOINT, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:57:01.102456Z",
     "iopub.status.busy": "2025-03-31T15:57:01.102109Z",
     "iopub.status.idle": "2025-03-31T15:57:01.108266Z",
     "shell.execute_reply": "2025-03-31T15:57:01.107327Z",
     "shell.execute_reply.started": "2025-03-31T15:57:01.102426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define model with classification head\n",
    "class DistilBertForClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(DistilBertForClassification, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(MODEL_CHECKPOINT, config=config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()  # Add loss function\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # CLS token representation\n",
    "\n",
    "        # Compute loss when labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:57:05.538039Z",
     "iopub.status.busy": "2025-03-31T15:57:05.537673Z",
     "iopub.status.idle": "2025-03-31T15:57:07.070058Z",
     "shell.execute_reply": "2025-03-31T15:57:07.069178Z",
     "shell.execute_reply.started": "2025-03-31T15:57:05.538008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b95061c2df04ceb83a7f1d6cea42518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForClassification(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:57:17.227708Z",
     "iopub.status.busy": "2025-03-31T15:57:17.227364Z",
     "iopub.status.idle": "2025-03-31T15:57:27.253610Z",
     "shell.execute_reply": "2025-03-31T15:57:27.252712Z",
     "shell.execute_reply.started": "2025-03-31T15:57:17.227675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a470f7a0b0452d8c83cf286f5cbee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b95f3b17f84b40b94caf07c3c16598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296b54f2e4754a64a98aff77b43305b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e04eb81e2564e68828a77703d1c08a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37419293f364924848e61a409592510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23234f9c7164224b92dc972683f0a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87da37c1554542edaa53dc5d36486c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:57:27.255100Z",
     "iopub.status.busy": "2025-03-31T15:57:27.254853Z",
     "iopub.status.idle": "2025-03-31T16:05:53.629728Z",
     "shell.execute_reply": "2025-03-31T16:05:53.628818Z",
     "shell.execute_reply.started": "2025-03-31T15:57:27.255078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575d74d5f7e84713933bc079637314bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c246b73262374b85b856584127272b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f66b6aaeb00498cafd561eea89cc90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:07:13.271512Z",
     "iopub.status.busy": "2025-03-31T16:07:13.271185Z",
     "iopub.status.idle": "2025-03-31T16:07:13.303252Z",
     "shell.execute_reply": "2025-03-31T16:07:13.302389Z",
     "shell.execute_reply.started": "2025-03-31T16:07:13.271490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:07:21.170335Z",
     "iopub.status.busy": "2025-03-31T16:07:21.169979Z",
     "iopub.status.idle": "2025-03-31T16:07:21.343795Z",
     "shell.execute_reply": "2025-03-31T16:07:21.343138Z",
     "shell.execute_reply.started": "2025-03-31T16:07:21.170304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:07:23.601630Z",
     "iopub.status.busy": "2025-03-31T16:07:23.601312Z",
     "iopub.status.idle": "2025-03-31T16:52:57.025802Z",
     "shell.execute_reply": "2025-03-31T16:52:57.024866Z",
     "shell.execute_reply.started": "2025-03-31T16:07:23.601603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250331_160723-psi0bae8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mihika-usgaonker-none/huggingface/runs/psi0bae8' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/mihika-usgaonker-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mihika-usgaonker-none/huggingface' target=\"_blank\">https://wandb.ai/mihika-usgaonker-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mihika-usgaonker-none/huggingface/runs/psi0bae8' target=\"_blank\">https://wandb.ai/mihika-usgaonker-none/huggingface/runs/psi0bae8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 45:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286400</td>\n",
       "      <td>0.292394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.257574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.358047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_path = \"./fine_tuned_distilbert\"\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:56:27.339587Z",
     "iopub.status.busy": "2025-03-31T16:56:27.339277Z",
     "iopub.status.idle": "2025-03-31T16:56:27.347246Z",
     "shell.execute_reply": "2025-03-31T16:56:27.346287Z",
     "shell.execute_reply.started": "2025-03-31T16:56:27.339561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "example_text = \"I loved the food.\"\n",
    "inputs = tokenizer(example_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T16:56:29.633909Z",
     "iopub.status.busy": "2025-03-31T16:56:29.633547Z",
     "iopub.status.idle": "2025-03-31T16:56:29.645421Z",
     "shell.execute_reply": "2025-03-31T16:56:29.644708Z",
     "shell.execute_reply.started": "2025-03-31T16:56:29.633878Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract logits and get predicted class\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7011277,
     "sourceId": 11225750,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
